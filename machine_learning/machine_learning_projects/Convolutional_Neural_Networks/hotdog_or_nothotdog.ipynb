{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__vk2kbCJv0C"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0s4YXNYqvMZJ"
      },
      "source": [
        "## Citation\n",
        "\n",
        "```\n",
        "@inproceedings{bossard14,\n",
        "  title = {Food-101 -- Mining Discriminative Components with Random Forests},\n",
        "  author = {Bossard, Lukas and Guillaumin, Matthieu and Van Gool, Luc},\n",
        "  booktitle = {European Conference on Computer Vision},\n",
        "  year = {2014}\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjye-S8wKoWk"
      },
      "source": [
        "#Data\n",
        "TensorFlow food 101 dataset.\n",
        "\n",
        "https://www.tensorflow.org/datasets/catalog/food101\n",
        "\n",
        "label of Hotdog is 55"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4P-MXyKeK_HB"
      },
      "outputs": [],
      "source": [
        "ds, ds_info = tfds.load('food101', shuffle_files=True, as_supervised=True, with_info=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>code explanation</summary>\n",
        "\n",
        "```tfds.load('food101', shuffle_files=True, as_supervised=True, with_info=True):```\n",
        "\n",
        "This function is used to load the 'food101' dataset from TensorFlow Datasets (tfds).\n",
        "\n",
        "Parameters:\n",
        "'food101': Specifies the name of the dataset to be loaded.\n",
        "\n",
        "shuffle_files=True: Specifies that the dataset files should be shuffled.\n",
        "\n",
        "as_supervised=True: Loads the dataset in a supervised format, where inputs and labels are returned as tuples.\n",
        "\n",
        "with_info=True: Retrieves additional information about the dataset, such as the number of classes, size, etc.\n",
        "\n",
        "Returns:\n",
        "\n",
        "ds: The loaded dataset.\n",
        "\n",
        "ds_info: Additional information about the dataset.\n",
        "</details>"
      ],
      "metadata": {
        "id": "v5cxTznAlajp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUD83UF4-FA-"
      },
      "outputs": [],
      "source": [
        "train_ds, valid_ds = ds[\"train\"], ds[\"validation\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q90IsCXR-dDB"
      },
      "outputs": [],
      "source": [
        "fig = tfds.show_examples(train_ds, ds_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qx8ChZk1-1NO"
      },
      "outputs": [],
      "source": [
        "MAX_SIDE_LEN = 128\n",
        "HOT_DOG_CLASS = 55\n",
        "train_ds = train_ds.map(\n",
        "    lambda image, label: (tf.cast(tf.image.resize(image, [MAX_SIDE_LEN, MAX_SIDE_LEN]), dtype=tf.int32), tf.cast(label == HOT_DOG_CLASS, dtype=tf.int32))\n",
        ")\n",
        "valid_ds = valid_ds.map(\n",
        "    lambda image, label: (tf.cast(tf.image.resize(image, [MAX_SIDE_LEN, MAX_SIDE_LEN]), dtype=tf.int32), tf.cast(label == HOT_DOG_CLASS, dtype=tf.int32))\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>code explanation</summary>\n",
        "\n",
        "```tf.data.Dataset.map(lambda image, label: (..., ...)):```\n",
        "\n",
        "This function is used to apply a mapping function to each element of a dataset.\n",
        "\n",
        "In this code, it is used twice to preprocess the training and validation datasets:\n",
        "\n",
        "It resizes the images to a maximum side length of 128 pixels using\n",
        "```tf.image.resize.```\n",
        "\n",
        "It converts the labels of hotdogs (class 55) to 1 and other classes to 0 using ```tf.cast.```\n",
        "</details>"
      ],
      "metadata": {
        "id": "2r-Np_pJmlQe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXOwltJABVXx"
      },
      "outputs": [],
      "source": [
        "fig = tfds.show_examples(train_ds, ds_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDtszsoECX8k"
      },
      "outputs": [],
      "source": [
        "train_hd_size, valid_hd_size = 750, 250\n",
        "train_hotdogs = train_ds.filter(lambda image, label: label == 1).repeat(3)\n",
        "train_nothotdogs = train_ds.filter(lambda image, label: label == 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4NdstrBD8R7"
      },
      "outputs": [],
      "source": [
        "valid_hotdogs = train_ds.filter(lambda image, label: label == 1).repeat(3)\n",
        "valid_nothotdogs = train_ds.filter(lambda image, label: label == 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>code explanation</summary>\n",
        "\n",
        "```tf.data.Dataset.filter(lambda image, label: ...):```\n",
        "\n",
        "This function is used to filter the elements of a dataset based on a given condition.\n",
        "\n",
        "In this code, it is used to create separate datasets for hotdogs and not hotdogs in the training and validation sets.\n",
        "</details>"
      ],
      "metadata": {
        "id": "JVNpN9wHnKFT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ht5Qr8pLF6No"
      },
      "outputs": [],
      "source": [
        "batch_size = 16\n",
        "train_ds = tf.data.Dataset.sample_from_datasets([train_hotdogs, train_nothotdogs],[0.5,0.5], stop_on_empty_dataset=True)\n",
        "train_ds = train_ds.cache().batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "valid_ds = tf.data.Dataset.sample_from_datasets([valid_hotdogs, valid_nothotdogs],[0.5,0.5], stop_on_empty_dataset=True)\n",
        "valid_ds = valid_ds.cache().batch(batch_size).prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>code explanation</summary>\n",
        "```tf.data.Dataset.sample_from_datasets([dataset1, dataset2], [weights1, weights2], stop_on_empty_dataset=True):```\n",
        "\n",
        "This function is used to sample elements from multiple datasets based on the provided weights.\n",
        "\n",
        "In this code, it is used to create balanced training and validation datasets by sampling equal proportions of hotdog and not hotdog samples.\n",
        "\n",
        "```tf.data.Dataset.cache():```\n",
        "\n",
        "This function is used to cache the elements of a dataset in memory or on disk.\n",
        "\n",
        "In this code, it is used to cache the training and validation datasets, which can improve training performance by avoiding repeated data loading and preprocessing.\n",
        "\n",
        "```tf.data.Dataset.batch(batch_size):```\n",
        "\n",
        "This function is used to combine consecutive elements of a dataset into batches.\n",
        "In this code, it is used to create batches of a specified size for training and validation datasets.\n",
        "\n",
        "```tf.data.Dataset.prefetch(tf.data.AUTOTUNE):```\n",
        "\n",
        "This function is used to optimize data loading by prefetching elements from a dataset while the model is training on the current batch.\n",
        "In this code, it is used to prefetch elements from the training and validation datasets to improve training performance.\n",
        "</details>"
      ],
      "metadata": {
        "id": "IcIeqC1LnTsC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcvFJ_6PH5WD"
      },
      "outputs": [],
      "source": [
        "for image_batch, label_batch in train_ds.take(1):\n",
        "  print(image_batch)\n",
        "  print(label_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXifKvdMD7gO"
      },
      "source": [
        "#Neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emac1DDD5uhV"
      },
      "outputs": [],
      "source": [
        "data_augumentaion = tf.keras.Sequential([\n",
        "    tf.keras.layers.RandomFlip('horizontal'),\n",
        "    tf.keras.layers.RandomRotation(0.2)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>code explanation</summary>\n",
        "```tf.keras.Sequential():```\n",
        "\n",
        "This function is used to create a sequential model in Keras.\n",
        "\n",
        "In this code, it is used to define the neural network model architecture.\n",
        "\n",
        "```tf.keras.layers.RandomFlip('horizontal'):```\n",
        "\n",
        "This function is used to randomly flip the images horizontally during training for data augmentation.\n",
        "\n",
        "It helps the model generalize better by providing additional variations of the training data.\n",
        "\n",
        "```tf.keras.layers.RandomRotation(0.2):```\n",
        "\n",
        "This function is used to randomly rotate the images by a maximum angle of 0.2 radians during training for data augmentation.\n",
        "\n",
        "It introduces further variations in the training data, making the model more robust to rotation.\n",
        "\n",
        "<details>"
      ],
      "metadata": {
        "id": "GPmuIxfostIx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i,_ in ds[\"train\"].take(1):\n",
        "  image = i"
      ],
      "metadata": {
        "id": "A0VLzeSwseSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68Ib0M_d6pW5"
      },
      "outputs": [],
      "source": [
        "plt.imshow(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cA10rJNw7faZ"
      },
      "outputs": [],
      "source": [
        "image = tf.cast(tf.expand_dims(image,0), tf.float32)\n",
        "image /= 255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o173RpEL7v2X"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "for i in range(9):\n",
        "  augumented_image = data_augumentaion(image)\n",
        "  ax = plt.subplot(3,3, i + 1)\n",
        "  plt.imshow(augumented_image[0])\n",
        "  plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7NJn6-rfCl_y"
      },
      "outputs": [],
      "source": [
        "random.seed(0)\n",
        "model = models.Sequential()\n",
        "model.add(layers.Rescaling(1./255))\n",
        "model.add(data_augumentaion)\n",
        "model.add(layers.Conv2D(64, (3,3), activation=\"relu\", input_shape=[MAX_SIDE_LEN, MAX_SIDE_LEN, 3]))\n",
        "model.add(layers.MaxPooling2D(2,2))\n",
        "model.add(layers.Dropout(0.25))\n",
        "model.add(layers.Conv2D(64, (3,3), activation=\"relu\", kernel_regularizer = tf.keras.regularizers.l2(l = 0.1)))\n",
        "model.add(layers.MaxPooling2D(2,2))\n",
        "model.add(layers.Dropout(0.25))\n",
        "model.add(layers.Conv2D(32, (3,3), activation=\"relu\", kernel_regularizer = tf.keras.regularizers.l2(l = 0.1)))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(128, activation=\"relu\"))\n",
        "model.add(layers.Dropout(0.25))\n",
        "model.add(layers.Dense(1))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>code explanation</summary>\n",
        "```tf.keras.layers.Rescaling(1./255):```\n",
        "\n",
        "This layer is used to normalize the pixel values of the input images between 0 and 1 by dividing them by 255.\n",
        "\n",
        "Normalizing the input data helps in faster convergence during training.\n",
        "\n",
        "```tf.keras.layers.Conv2D(filters, kernel_size, activation, input_shape):```\n",
        "\n",
        "This layer is used to add a 2D convolutional layer to the model.\n",
        "\n",
        "It performs convolutional operations on the input data to extract features.\n",
        "Parameters:\n",
        "\n",
        "filters: The number of filters in the convolutional layer.\n",
        "\n",
        "kernel_size: The size of the filters.\n",
        "\n",
        "activation: The activation function to be applied.\n",
        "\n",
        "input_shape: The shape of the input data.\n",
        "\n",
        "```tf.keras.layers.MaxPooling2D(pool_size):```\n",
        "\n",
        "This layer is used to add a max pooling layer to the model.\n",
        "\n",
        "It downsamples the input data by taking the maximum value within each pooling window.\n",
        "\n",
        "Parameter:\n",
        "\n",
        "pool_size: The size of the pooling window.\n",
        "\n",
        "```tf.keras.layers.Dropout(rate):```\n",
        "\n",
        "This layer is used to apply dropout regularization to the model.\n",
        "\n",
        "It randomly sets a fraction of input units to 0 during training, which helps in reducing overfitting.\n",
        "\n",
        "Parameter:\n",
        "\n",
        "rate: The fraction of input units to drop.\n",
        "\n",
        "```tf.keras.layers.Flatten():```\n",
        "\n",
        "This layer is used to flatten the multi-dimensional input into a 1D vector.\n",
        "\n",
        "It prepares the data for the fully connected layers.\n",
        "\n",
        "```tf.keras.layers.Dense(units, activation):```\n",
        "\n",
        "This layer is used to add a fully connected layer to the model.\n",
        "\n",
        "It connects every input unit to every output unit.\n",
        "\n",
        "Parameters:\n",
        "\n",
        "units: The number of neurons in the layer.\n",
        "\n",
        "activation: The activation function to be applied.\n",
        "</details>"
      ],
      "metadata": {
        "id": "VK5O0uNPtP66"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPqujz21pVKs"
      },
      "outputs": [],
      "source": [
        "lr = 0.0001\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(lr),\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>code explanation</summary>\n",
        "\n",
        "```tf.keras.optimizers.Adam(learning_rate):```\n",
        "\n",
        "This optimizer is used to update the model's parameters during training using the Adam optimization algorithm.\n",
        "\n",
        "It adapts the learning rate based on the moving averages of the gradient.\n",
        "\n",
        "Parameter:\n",
        "\n",
        "learning_rate: The learning rate for the optimizer.\n",
        "\n",
        "```tf.keras.losses.BinaryCrossentropy(from_logits=True):```\n",
        "\n",
        "This loss function is used for binary classification problems.\n",
        "\n",
        "It computes the cross-entropy loss between the true labels and predicted logits.\n",
        "\n",
        "Parameter:\n",
        "\n",
        "from_logits=True: Specifies that the model's output is not normalized with a sigmoid activation.\n",
        "</details>"
      ],
      "metadata": {
        "id": "UTQiEsMuudh6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "YPOZs7CHtSt2"
      },
      "outputs": [],
      "source": [
        "epochs = 15\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=valid_ds,\n",
        "    epochs=epochs,\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<details>\n",
        "<summary>code explanation</summary>\n",
        "\n",
        "```model.fit(train_ds, validation_data, epochs, verbose):```\n",
        "\n",
        "This function is used to train the model.\n",
        "\n",
        "It fits the model to the training data and evaluates it on the validation data for the specified number of epochs.\n",
        "\n",
        "Parameters:\n",
        "\n",
        "train_ds: The training dataset.\n",
        "\n",
        "validation_data: The validation dataset.\n",
        "\n",
        "epochs: The number of epochs for training.\n",
        "\n",
        "verbose: Controls the verbosity mode (0 = silent, 1 = progress bar, 2 = one line per epoch).\n",
        "\n",
        "```plt.imshow(image)``` and ```plt.show():```\n",
        "\n",
        "These functions are used to display an image using matplotlib.\n",
        "\n",
        "They are used to visualize the images from the dataset and the predicted labels.\n",
        "</details>"
      ],
      "metadata": {
        "id": "fbuo6hB4v6O9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgJE8-3F0-YV"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "for image_batch, label_batch in valid_ds.take(1):\n",
        "  images = image_batch\n",
        "  labels = label_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRnbD87K1U04"
      },
      "outputs": [],
      "source": [
        "for i in range(9):\n",
        "  ax = plt.subplot(3,3, i+1)\n",
        "  plt.imshow(images[i])\n",
        "  plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQgNHagN2DCL"
      },
      "outputs": [],
      "source": [
        "labels[:9]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
