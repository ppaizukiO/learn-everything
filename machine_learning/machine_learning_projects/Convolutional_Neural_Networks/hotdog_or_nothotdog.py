# -*- coding: utf-8 -*-
"""hotdog_or_nothotdog

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HYr4pespbuS4v-gqOGy1H2i0N5lC3DUI
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import random
import tensorflow as tf
from tensorflow.keras import datasets, layers, models

import tensorflow_datasets as tfds

"""## Citation

```
@inproceedings{bossard14,
  title = {Food-101 -- Mining Discriminative Components with Random Forests},
  author = {Bossard, Lukas and Guillaumin, Matthieu and Van Gool, Luc},
  booktitle = {European Conference on Computer Vision},
  year = {2014}
}
```

#Data
TensorFlow food 101 dataset.

https://www.tensorflow.org/datasets/catalog/food101

label of Hotdog is 55
"""

ds, ds_info = tfds.load('food101', shuffle_files=True, as_supervised=True, with_info=True)


train_ds, valid_ds = ds["train"], ds["validation"]

fig = tfds.show_examples(train_ds, ds_info)

MAX_SIDE_LEN = 128
HOT_DOG_CLASS = 55
train_ds = train_ds.map(
    lambda image, label: (tf.cast(tf.image.resize(image, [MAX_SIDE_LEN, MAX_SIDE_LEN]), dtype=tf.int32), tf.cast(label == HOT_DOG_CLASS, dtype=tf.int32))
)
valid_ds = valid_ds.map(
    lambda image, label: (tf.cast(tf.image.resize(image, [MAX_SIDE_LEN, MAX_SIDE_LEN]), dtype=tf.int32), tf.cast(label == HOT_DOG_CLASS, dtype=tf.int32))
)


fig = tfds.show_examples(train_ds, ds_info)

train_hd_size, valid_hd_size = 750, 250
train_hotdogs = train_ds.filter(lambda image, label: label == 1).repeat(3)
train_nothotdogs = train_ds.filter(lambda image, label: label == 0)

valid_hotdogs = train_ds.filter(lambda image, label: label == 1).repeat(3)
valid_nothotdogs = train_ds.filter(lambda image, label: label == 0)


batch_size = 16
train_ds = tf.data.Dataset.sample_from_datasets([train_hotdogs, train_nothotdogs],[0.5,0.5], stop_on_empty_dataset=True)
train_ds = train_ds.cache().batch(batch_size).prefetch(tf.data.AUTOTUNE)

valid_ds = tf.data.Dataset.sample_from_datasets([valid_hotdogs, valid_nothotdogs],[0.5,0.5], stop_on_empty_dataset=True)
valid_ds = valid_ds.cache().batch(batch_size).prefetch(tf.data.AUTOTUNE)


for image_batch, label_batch in train_ds.take(1):
  print(image_batch)
  print(label_batch)

"""#Neural network"""

data_augumentaion = tf.keras.Sequential([
    tf.keras.layers.RandomFlip('horizontal'),
    tf.keras.layers.RandomRotation(0.2)
])


for i,_ in ds["train"].take(1):
  image = i

plt.imshow(image)

image = tf.cast(tf.expand_dims(image,0), tf.float32)
image /= 255.0

plt.figure(figsize=(10,10))
for i in range(9):
  augumented_image = data_augumentaion(image)
  ax = plt.subplot(3,3, i + 1)
  plt.imshow(augumented_image[0])
  plt.axis("off")

random.seed(0)
model = models.Sequential()
model.add(layers.Rescaling(1./255))
model.add(data_augumentaion)
model.add(layers.Conv2D(64, (3,3), activation="relu", input_shape=[MAX_SIDE_LEN, MAX_SIDE_LEN, 3]))
model.add(layers.MaxPooling2D(2,2))
model.add(layers.Dropout(0.25))
model.add(layers.Conv2D(64, (3,3), activation="relu", kernel_regularizer = tf.keras.regularizers.l2(l = 0.1)))
model.add(layers.MaxPooling2D(2,2))
model.add(layers.Dropout(0.25))
model.add(layers.Conv2D(32, (3,3), activation="relu", kernel_regularizer = tf.keras.regularizers.l2(l = 0.1)))
model.add(layers.Flatten())
model.add(layers.Dense(128, activation="relu"))
model.add(layers.Dropout(0.25))
model.add(layers.Dense(1))


lr = 0.0001
model.compile(optimizer=tf.keras.optimizers.Adam(lr),
              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
              metrics=["accuracy"])


epochs = 15
history = model.fit(
    train_ds,
    validation_data=valid_ds,
    epochs=epochs,
    verbose=1
)


plt.figure(figsize=(10,10))
for image_batch, label_batch in valid_ds.take(1):
  images = image_batch
  labels = label_batch

for i in range(9):
  ax = plt.subplot(3,3, i+1)
  plt.imshow(images[i])
  plt.axis("off")

labels[:9]
